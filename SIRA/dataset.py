import torch.utils.data as data
import pickle
import os
import torch
import random
import numpy as np
import open3d as o3d
from typing import Dict


def uniform_sample_rotation() -> np.ndarray:
    """Random rotation matrix generated from QR decomposition

    Rotation generated by this function is uniformly distributed on SO(3) w.r.t Haar measure

    NOTE: RRE of the rotation generated by this function is NOT uniformly distributed
    
    """
    # QR decomposition
    z = np.random.randn(3, 3)
    while np.linalg.matrix_rank(z) != z.shape[0]:
        z = np.random.randn(3, 3)
    q, r = np.linalg.qr(z)
    d = np.diag(r)
    ph = np.diag(d / np.absolute(d))
    q = np.matmul(q, ph)
    # # if det(rotation) == -1, project q to so(3)
    # #rotation = np.linalg.det(q) * q
    rotation = q / np.linalg.det(q)

    return rotation


class UnalignedDataset(data.Dataset):

    def __init__(self,
                 root="./dataset/synth2real",
                 real_log="3DMatch_train.log",
                 synth_log="FlyingShapes_tsdf.log",
                 real_data_root="../PCR/dataset/3DMatch/data/train",
                 synth_data_root="../PCR/dataset/FlyingShapes",
                 synth_scene_num=25,
                 point_limit=30000):
        super(UnalignedDataset, self).__init__()
        self.root = root
        self.point_limit = point_limit

        self.synth_root = os.path.join(root, "synth")
        self.real_root = os.path.join(root, "real")

        self.synth_list = []
        with open(os.path.join(self.synth_root, synth_log), "r") as f:
            for line in f.readlines():
                data = {}
                scene_index, ply_path, _ = line.split("\t")
                scene_index = int(scene_index)
                if scene_index >= synth_scene_num:
                    continue
                data["scene_name"] = "scene_index_" + str(scene_index)
                data["ply_path"] = os.path.join(synth_data_root, ply_path)
                self.synth_list.append(data)

        self.real_list = []
        with open(os.path.join(self.real_root, real_log), "r") as f:
            for line in f.readlines():
                data = {}
                scene_index, ply_path, _ = line.split("\t")
                path_splits = ply_path.split("/")
                scene_name = path_splits[-2]
                data["scene_name"] = scene_name
                data["ply_path"] = os.path.join(real_data_root, ply_path)
                self.real_list.append(data)

        self.synth_size = len(self.synth_list)  # get the size of dataset synth
        self.real_size = len(self.real_list)  # get the size of dataset real

    def _load_point_cloud(self, file_path):
        if file_path.endswith('.pth'):
            points = torch.load(file_path)
        elif file_path.endswith('.ply'):
            pcd = o3d.io.read_point_cloud(file_path)
            points = np.asarray(pcd.points)
        elif file_path.endswith('.bin'):
            points = np.fromfile(file_path, dtype=np.float32).reshape(-1, 4)
        else:
            raise AssertionError('Cannot recognize point cloud format')

        return points

    def _point_limit(self, points: np.ndarray):
        if points.shape[0] > self.point_limit:
            indices = np.random.permutation(points.shape[0])[:self.point_limit]
            points = points[indices]
        return points

    def _augment(self, points: np.ndarray):
        centroid = points.mean(axis=0)
        points = points - centroid

        rot = uniform_sample_rotation()
        points = points @ rot.T

        return points

    def __getitem__(self, index):
        data_dict = {}

        synth_metadata: Dict = self.synth_list[
            index % self.synth_size]  # make sure the index is within the range
        real_metadata: Dict = self.real_list[np.random.randint(
            self.real_size
        )]  # randomize the index for domain B to avoid fixed pairs.

        # load synth data
        data_dict["synth_scene_name"] = synth_metadata["scene_name"]

        # get point cloud
        synth_path = synth_metadata["ply_path"]
        synth_points = self._load_point_cloud(synth_path)
        data_dict["synth_path"] = synth_path

        # point limit
        synth_points = self._point_limit(synth_points)

        # load real data
        data_dict["real_scene_name"] = real_metadata["scene_name"]

        # get point cloud
        real_path = real_metadata["ply_path"]
        real_points = self._load_point_cloud(real_path)
        data_dict["real_path"] = real_path

        # point limit
        real_points = self._point_limit(real_points)

        # augmentation
        synth_points = self._augment(synth_points)
        real_points = self._augment(real_points)

        data_dict["synth"] = synth_points.astype(np.float32)
        data_dict["real"] = real_points.astype(np.float32)

        return data_dict

    def __len__(self):
        return max(self.synth_size, self.real_size)


class TestDataset(data.Dataset):

    def __init__(self,
                 root="./dataset/synth2real",
                 synth_log="FlyingShapes_tsdf.log",
                 synth_data_root="../PCR/dataset/FlyingShapes",
                 synth_scene_num=25,
                 point_limit=30000):
        super(TestDataset, self).__init__()
        self.root = root
        self.point_limit = point_limit

        self.synth_root = os.path.join(root, "synth")

        self.synth_list = []
        with open(os.path.join(self.synth_root, synth_log), "r") as f:
            for line in f.readlines():
                data = {}
                scene_index, ply_path, _ = line.split("\t")
                scene_index = int(scene_index)
                if scene_index >= synth_scene_num:
                    continue
                data["scene_name"] = "scene_index_" + str(scene_index)
                data["ply_path"] = os.path.join(synth_data_root, ply_path)
                self.synth_list.append(data)

        self.synth_size = len(self.synth_list)  # get the size of dataset synth

    def _load_point_cloud(self, file_path):
        if file_path.endswith('.pth'):
            points = torch.load(file_path)
        elif file_path.endswith('.ply'):
            pcd = o3d.io.read_point_cloud(file_path)
            points = np.asarray(pcd.points)
        elif file_path.endswith('.bin'):
            points = np.fromfile(file_path, dtype=np.float32).reshape(-1, 4)
        else:
            raise AssertionError('Cannot recognize point cloud format')

        return points

    def _point_limit(self, points: np.ndarray):
        if points.shape[0] > self.point_limit:
            indices = np.random.permutation(points.shape[0])[:self.point_limit]
            points = points[indices]
        return points

    def _augment(self, points: np.ndarray):
        centroid = points.mean(axis=0)
        points = points - centroid

        rot = uniform_sample_rotation()
        points = points @ rot.T

        return points, rot, centroid

    def __getitem__(self, index):
        data_dict = {}

        synth_metadata: Dict = self.synth_list[
            index % self.synth_size]  # make sure the index is within the range
        # load synth data
        data_dict["synth_scene_name"] = synth_metadata["scene_name"]

        # get point cloud
        synth_path = synth_metadata["ply_path"]
        synth_points = self._load_point_cloud(synth_path)
        data_dict["synth_path"] = synth_path

        # point limit
        synth_points = self._point_limit(synth_points)

        # augmentation
        synth_points, synth_rot, synth_centroid = self._augment(synth_points)

        data_dict["synth"] = synth_points.astype(np.float32)
        data_dict["synth_rot"] = synth_rot.astype(np.float32)
        data_dict["synth_centroid"] = synth_centroid.astype(np.float32)

        return data_dict

    def __len__(self):
        return self.synth_size